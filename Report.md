# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ3 ‚Äî Streaming processing —Å –ø–æ–º–æ—â—å—é Flink
**–°—Ç—É–¥–µ–Ω—Ç:** *–ê–ª–∞–ø–∞–Ω–æ–≤–∞ –≠–ª—å–∑–∞*
**–ì—Ä—É–ø–ø–∞:** *–ú8–û-209–°–í-24*

---

## –¶–µ–ª—å —Ä–∞–±–æ—Ç—ã

–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø–æ—Ç–æ–∫–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Apache Flink.

## –¢–µ–∫–Ω–æ–ª–æ–¥–∂–∏–∏

* **Apache Flink** (–≤–µ—Ä—Å–∏–∏ 1.17.2, Python 3.10) ‚Äî –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
* **Apache Kafka** (–≤–µ—Ä—Å–∏–∏ 7.5.0) ‚Äî –±—Ä–æ–∫–µ—Ä –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
* **PostgreSQL** (–≤–µ—Ä—Å–∏—è 15) ‚Äî —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
* **Docker/Docker Compose** ‚Äî –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
* **Python** ‚Äî —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Kafka Producer –∏ Flink job
* **pyflink** ‚Äî Python API –¥–ª—è Flink

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–µ—à–µ–Ω–∏—è

–°–µ—Ä–≤–∏—Å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ —á–µ—Ä–µ–∑ Docker Compose:

| –°–µ—Ä–≤–∏—Å         | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ          | –ü–æ—Ä—Ç | –ü—Ä–∏–º–µ—á–∞–Ω–∏—è                                          |
| -------------- | ------------------- | ---- | --------------------------------------------------- |
| zookeeper      | –ö–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è Kafka   | 2181 | –ë—Ä–æ–∫–µ—Ä Zookeeper                                    |
| kafka          | Kafka broker        | 9092 | –•—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ø–µ—Ä–µ–¥–∞—á–∞ —Å–æ–æ–±—â–µ–Ω–∏–π                       |
| postgres       | PostgreSQL          | 5432 | –•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è —Ç–∞–±–ª–∏—Ü –º–æ–¥–µ–ª–∏ ¬´–∑–≤–µ–∑–¥–∞¬ª                |
| jobmanager     | Flink JobManager    | 8081 | –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ Flink-–∫–ª–∞—Å—Ç–µ—Ä–æ–º                          |
| taskmanager    | Flink TaskManager   | ‚Äî    | –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Ç–æ–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö                            |
| kafka-producer | –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–æ–æ–±—â–µ–Ω–∏–π | ‚Äî    | –ß–∏—Ç–∞–µ—Ç CSV –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç JSON –≤ Kafka                |
| flink-job      | Flink job submitter | ‚Äî    | –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ—Ç–æ–∫–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∏ –∑–∞–ø–∏—Å—å –≤ PostgreSQL |

–°—Ö–µ–º–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è:

```
CSV files ‚Üí Kafka Producer ‚Üí Kafka Topic ‚Üí Flink Job ‚Üí PostgreSQL (Star Schema)
```

``` yaml
services:
  # Zookeeper –¥–ª—è Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - flink-network

  # Kafka broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    networks:
      - flink-network

  # PostgreSQL
  postgres:
    image: postgres:15
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: lab3
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - flink-network

  # Flink JobManager
  jobmanager:
    container_name: jobmanager
    hostname: jobmanager
    command: ["jobmanager"]
    image: ghcr.io/lakehq/flink:1.17.2-python3.10
    entrypoint: ["/docker-entrypoint.sh"]
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
      - POSTGRES_URL=postgresql://admin:admin123@postgres:5432/lab3
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=admin123
      - KAFKA_BOOTSTRAP=kafka:9092
      - KAFKA_TOPIC=pet-sales
    ports: ["8081:8081"]
    networks:
      - flink-network
    volumes:
      - ./flink_job:/opt/flink/usrlib

  # Flink TaskManager
  taskmanager:
    container_name: taskmanager
    image: ghcr.io/lakehq/flink:1.17.2-python3.10
    hostname: taskmanager
    command: ["taskmanager"]
    depends_on: [jobmanager]
    entrypoint: ["/docker-entrypoint.sh"]
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
      - TASK_MANAGER_NUMBER_OF_TASK_SLOTS=4
      - POSTGRES_URL=postgresql://admin:admin123@postgres:5432/lab3
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=admin123
      - KAFKA_BOOTSTRAP=kafka:9092
      - KAFKA_TOPIC=pet-sales
    networks:
      - flink-network
    volumes:
      - ./flink_job:/opt/flink/usrlib

  # Kafka Producer
  kafka-producer:
    build:
      context: ./kafka_prod
      dockerfile: Dockerfile
    container_name: kafka-producer
    depends_on:
      - kafka
      - postgres
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:9092
      KAFKA_TOPIC: pet-sales
      MESSAGE_DELAY: "0.05"
    volumes:
      - ./data:/app/data
    networks:
      - flink-network
    command: ["python", "producer.py"]
    restart: on-failure

  # Flink Job Submitter
  flink-job:
    build: ./flink_job
    platform: linux/amd64
    depends_on:
      - kafka
      - postgres
      - jobmanager
      - taskmanager
      - kafka-producer
    entrypoint: ["bash", "-c"]
    command:
      - |
        echo "Waiting for Kafka to be ready..."
        sleep 45
        echo "Waiting for Flink cluster to be ready..."
        sleep 15
        echo "Submitting Flink job..."
        flink run -m jobmanager:8081 -d --python /opt/flink/job/flink_streaming_job.py
        echo "Job submitted!"
    environment:
      POSTGRES_URL: postgresql://admin:admin123@postgres:5432/lab3
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin123
      KAFKA_BOOTSTRAP: kafka:9092
      KAFKA_TOPIC: pet-sales
    networks:
      - flink-network
    volumes:
      - ./flink_job:/opt/flink/usrlib
    restart: "no"

networks:
  flink-network:
    driver: bridge

volumes:
  postgres-data:

```
## 7. –ó–∞–ø—É—Å–∫ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã

1. –°–±–æ—Ä–∫–∞ –æ–±—Ä–∞–∑–æ–≤:

```bash
docker-compose build
```

2. –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–æ–≤ Kafka –∏ PostgreSQL:

```bash
docker-compose up -d zookeeper kafka postgres
```

3. –°–æ–∑–¥–∞–Ω–∏–µ Kafka topic:

```bash
docker-compose exec kafka kafka-topics --create --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1 --topic pet-sales
```

4. –ó–∞–ø—É—Å–∫ Kafka Producer:

```bash
docker-compose up -d kafka-producer
```

5. –ó–∞–ø—É—Å–∫ Flink JobManager –∏ TaskManager:

```bash
docker-compose up -d jobmanager taskmanager
```

6. –ó–∞–ø—É—Å–∫ Flink job submitter:

```bash
docker-compose up -d flink-job
```

---

## –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

* 10 CSV-—Ñ–∞–π–ª–æ–≤ (`mock_data1.csv ‚Ä¶ mock_data10.csv`), –ø–æ 1000 —Å—Ç—Ä–æ–∫ –∫–∞–∂–¥—ã–π
* –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ JSON –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –≤ Kafka topic `pet-sales`

---


## –†–µ–∞–ª–∏–∑–∞—Ü–∏—è

### Kafka Producer

* –ß–∏—Ç–∞–µ—Ç CSV-—Ñ–∞–π–ª—ã –∏–∑ `./data`
* –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫–∏ –≤ JSON: –¥–æ–±–∞–≤–ª—è–µ—Ç `row_number`, `source_file`, `timestamp`
* –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ Kafka topic `pet-sales`
* –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ `MESSAGE_DELAY`

### Flink Streaming Job

* –ß–∏—Ç–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ Kafka topic `pet-sales`
* –ü–∞—Ä—Å–∏—Ç JSON-—Å–æ–æ–±—â–µ–Ω–∏—è
* –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –º–æ–¥–µ–ª—å ¬´–∑–≤–µ–∑–¥–∞¬ª:

  * Dimensional tables: `dim_customers`, `dim_sellers`, `dim_products`, `dim_stores`, `dim_suppliers`
  * Fact table: `fact_sales`
* –ü–∏—à–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ PostgreSQL —á–µ—Ä–µ–∑ JDBC
* –ò—Å–ø–æ–ª—å–∑—É–µ—Ç pyflink API, SQL-–∑–∞–ø—Ä–æ—Å—ã –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö

**–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**

* Flink UI: [http://localhost:8081](http://localhost:8081)
* –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–æ–≤:

```bash
docker-compose logs -f flink-job
```

–û—Ç–ª–∏—á–Ω–æ, —ç—Ç–æ –∫–∞–∫ —Ä–∞–∑ **–∫–ª—é—á–µ–≤–æ–π –ø—É–Ω–∫—Ç –æ—Ç—á—ë—Ç–∞**, –∏ —É —Ç–µ–±—è –æ–Ω —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ üëç
–ù–∏–∂–µ ‚Äî **–≥–æ—Ç–æ–≤—ã–π —Ä–∞–∑–¥–µ–ª 4 –¥–ª—è –æ—Ç—á—ë—Ç–∞**, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–Ω–æ **–≤—Å—Ç–∞–≤–ª—è—Ç—å –Ω–∞–ø—Ä—è–º—É—é** (—Ç–µ–∫—Å—Ç + –ø–æ—è—Å–Ω–µ–Ω–∏–µ + —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–¥–∞). –Ø –æ—Ñ–æ—Ä–º–ª—é –µ–≥–æ –≤ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–º —Å—Ç–∏–ª–µ, –∫–∞–∫ –æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É—é—Ç –Ω–∞ –õ–†.

---

## –ö–æ–¥ Apache Flink –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–∂–∏–º–µ streaming

–î–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω streaming-job –Ω–∞ **Apache Flink** —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **PyFlink (Table API + SQL)**.
–ó–∞–¥–∞—á–∞ Flink-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ‚Äî —Å—á–∏—Ç–∞—Ç—å –ø–æ—Ç–æ–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Kafka, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏—Ö –≤ –º–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö ¬´–∑–≤–µ–∑–¥–∞¬ª –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ PostgreSQL –≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.

---

### –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è Flink

–í –Ω–∞—á–∞–ª–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å–æ–∑–¥–∞—ë—Ç—Å—è –ø–æ—Ç–æ–∫–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (`StreamExecutionEnvironment`) –∏ —Ç–∞–±–ª–∏—á–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ (`StreamTableEnvironment`) –≤ streaming-—Ä–µ–∂–∏–º–µ:

```python
env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(2)

settings = EnvironmentSettings.new_instance() \
    .in_streaming_mode() \
    .build()

t_env = StreamTableEnvironment.create(env, settings)
```

–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ä–∞–≤–Ω—ã–º 2, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –ø–æ—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ.

---

### –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä–æ–≤ Kafka –∏ PostgreSQL

–î–ª—è —Ä–∞–±–æ—Ç—ã —Å Kafka –∏ PostgreSQL –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤–Ω–µ—à–Ω–∏–µ –∫–æ–Ω–Ω–µ–∫—Ç–æ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥–∫–ª—é—á–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä `pipeline.jars`:

```python
kafka_jar = 'file:///opt/flink/lib/flink-sql-connector-kafka-3.0.1-1.17.jar'
postgres_jar = 'file:///opt/flink/lib/postgresql-42.6.0.jar'
jdbc_jar = 'file:///opt/flink/lib/flink-connector-jdbc-3.1.1-1.17.jar'

t_env.get_config().get_configuration().set_string(
    "pipeline.jars",
    f"{kafka_jar};{postgres_jar};{jdbc_jar}"
)
```

–≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Kafka –∫–∞–∫ –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö, –∞ PostgreSQL ‚Äî –∫–∞–∫ –ø—Ä–∏—ë–º–Ω–∏–∫ (sink).

---

### –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö Kafka

–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–ø–∏—Å—ã–≤–∞—é—Ç—Å—è –∫–∞–∫ —Ç–∞–±–ª–∏—Ü–∞ `kafka_source`, –∫–æ—Ç–æ—Ä–∞—è —á–∏—Ç–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ Kafka topic `pet-sales` –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:

```sql
CREATE TABLE kafka_source (
    id STRING,
    customer_first_name STRING,
    customer_last_name STRING,
    customer_age STRING,
    customer_email STRING,
    customer_country STRING,
    customer_postal_code STRING,
    customer_pet_type STRING,
    customer_pet_name STRING,
    customer_pet_breed STRING,
    seller_first_name STRING,
    seller_last_name STRING,
    seller_email STRING,
    seller_country STRING,
    seller_postal_code STRING,
    product_name STRING,
    product_category STRING,
    product_price STRING,
    product_quantity STRING,
    sale_date STRING,
    sale_customer_id STRING,
    sale_seller_id STRING,
    sale_product_id STRING,
    sale_quantity STRING,
    sale_total_price STRING,
    store_name STRING,
    store_location STRING,
    store_city STRING,
    store_state STRING,
    store_country STRING,
    store_phone STRING,
    store_email STRING,
    pet_category STRING,
    product_weight STRING,
    product_color STRING,
    product_size STRING,
    product_brand STRING,
    product_material STRING,
    product_description STRING,
    product_rating STRING,
    product_reviews STRING,
    product_release_date STRING,
    product_expiry_date STRING,
    supplier_name STRING,
    supplier_contact STRING,
    supplier_email STRING,
    supplier_phone STRING,
    supplier_address STRING,
    supplier_city STRING,
    supplier_country STRING
) WITH (
    'connector' = 'kafka',
    'topic' = 'pet-sales',
    'properties.bootstrap.servers' = 'kafka:9092',
    'scan.startup.mode' = 'earliest-offset',
    'format' = 'json',
    'json.fail-on-missing-field' = 'false',
    'json.ignore-parse-errors' = 'true'
);
```

–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –∫–∞–∂–¥–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ Kafka –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ —Å—Ç—Ä–æ–∫—É —Ç–∞–±–ª–∏—Ü—ã Flink.

---

### –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –º–æ–¥–µ–ª–∏ ¬´–∑–≤–µ–∑–¥–∞¬ª –≤ PostgreSQL

–í PostgreSQL —Å–æ–∑–¥–∞—é—Ç—Å—è —Ç–∞–±–ª–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏–π (dimensions) –∏ —Ç–∞–±–ª–∏—Ü–∞ —Ñ–∞–∫—Ç–æ–≤ (fact):

* `dim_customers`
* `dim_sellers`
* `dim_products`
* `dim_stores`
* `dim_suppliers`
* `fact_sales`

–ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏–π:

```sql
CREATE TABLE IF NOT EXISTS dim_customers_sink (
    customer_id INT,
    customer_first_name STRING,
    customer_last_name STRING,
    customer_age INT,
    customer_email STRING,
    customer_country STRING,
    customer_postal_code STRING,
    customer_pet_type STRING,
    customer_pet_name STRING,
    customer_pet_breed STRING,
    PRIMARY KEY (customer_id) NOT ENFORCED
) WITH (
    'connector' = 'jdbc',
    'url' = 'jdbc:postgresql://postgres:5432/lab3',
    'table-name' = 'dim_customers',
    'username' = 'admin',
    'password' = 'admin123'
);
```

---

### –ü–æ—Ç–æ–∫–æ–≤–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö

–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –º–æ–¥–µ–ª—å ¬´–∑–≤–µ–∑–¥–∞¬ª –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é SQL-–∑–∞–ø—Ä–æ—Å–æ–≤ `INSERT INTO ‚Ä¶ SELECT`, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è **–≤ —Ä–µ–∂–∏–º–µ streaming**.

–ü—Ä–∏–º–µ—Ä –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã `dim_customers`:

```sql
INSERT INTO dim_customers_sink
SELECT DISTINCT
    CAST(sale_customer_id AS INT) AS customer_id,
    customer_first_name,
    customer_last_name,
    CAST(customer_age AS INT) AS customer_age,
    customer_email,
    customer_country,
    customer_postal_code,
    customer_pet_type,
    customer_pet_name,
    customer_pet_breed
FROM kafka_source
WHERE sale_customer_id IS NOT NULL AND sale_customer_id <> '';
```

–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã —Ñ–∞–∫—Ç–æ–≤ `fact_sales`:

```sql
INSERT INTO fact_sales_sink
SELECT
    CAST(sale_customer_id AS INT) AS customer_key,
    CAST(sale_seller_id AS INT) AS seller_key,
    CAST(sale_product_id AS INT) AS product_key,
    CAST(id AS INT) AS store_key,
    CAST(id AS INT) AS supplier_key,
    sale_date,
    CAST(sale_quantity AS INT) AS sale_quantity,
    CAST(sale_total_price AS DECIMAL(10,2)) AS sale_total_price,
    CAST(product_quantity AS INT) AS product_quantity
FROM kafka_source
WHERE id IS NOT NULL AND id <> '';
```

–í—Å–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ –ø–æ –º–µ—Ä–µ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ Kafka.

---

### –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

* –î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è **–≤ —Ä–µ–∂–∏–º–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏**
* –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è **Table API + SQL**, —á—Ç–æ —É–ø—Ä–æ—â–∞–µ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é ETL-–ª–æ–≥–∏–∫–∏
* –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –º–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö **Star Schema**
* Flink –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º

